{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Consider a convolutional neural network applied to an RGB input image of size $N\\times N$ where,\n",
    "for simplicity of analysis, $N$ is a power of 2. Suppose that\n",
    "\n",
    "\n",
    "• the convolutions each cover $k \\times k$ pixels,\n",
    "\n",
    "\n",
    "• there are $d$ different convolutions per convolution layer,\n",
    "\n",
    "\n",
    "• padding is used so that convolutions do not cause image shrinkage, and\n",
    "\n",
    "\n",
    "• after each convolution layer there is a max pooling layer applied over non-overlapping\n",
    "$2 \\times 2$ pixel regions.\n",
    "\n",
    "Suppose also that there $L$ convolution layers, followed by $F$ fully-connected layers with $h$\n",
    "nodes per layer, and $n^{(o)}$ nodes in the output layer. Derive an expression for the number\n",
    "of learnable convolution parameters and the number of learnable parameters in the fullyconnected and output layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each convolution has $$k * k + 1$$ parameters since we need 1 convolution kernel (of size $k\\times k$) that is used on every pixel of each image. We need the extra +1 because of the bias parameter.\n",
    "\n",
    "Originally, there are 3 input channels (RGB), but it immediately goes to $d$ output channels. From then on, there is always $d$ input and output channels. As a result, there are\n",
    "$$(3*k*k + 1)*d$$\n",
    "parameters in the first layer, and \n",
    "$$(d*k*k + 1)*d$$\n",
    "parameters in every subsequent layers.\n",
    "\n",
    "This means we have \n",
    "$$(3*k*k + 1)*d + L* \\left(d*k*k+1\\right)*d$$\n",
    "total convolutional parameters\n",
    "\n",
    "Note that pooling doesn't affect the convolutional parameters because every pixel of every image uses the same kernel regardless of how many pixels there are.\n",
    "\n",
    "Then we have to get the number of fully-connected parameters. The number of inputs to the first fully-connected layer is the number of outputs of the convolutional layers. \n",
    "The number of outputs of the convolutional layer is \n",
    "$$d*\\frac{N}{2^L}*\\frac{N}{2^L}$$\n",
    "because it goes through $L$ pooling layers that halve the image dimensions.\n",
    "This means the first fully-connected layer has \n",
    "$$d*\\frac{N}{2^L}*\\frac{N}{2^L}*h+h$$\n",
    "parameters since it has $d*\\frac{N}{2^L}*\\frac{N}{2^L}$ inputs, $h$ outputs, and $h$ bias parameters.\n",
    "\n",
    "Every subsequent fully connected layer has $h$ inputs, $h$ outputs, and $h$ bias parameters:\n",
    "$$h*h+h$$\n",
    "and there are $F-1$ of them (maybe there are $F-2$? It's ambiguous). This means there are\n",
    "$$d*\\frac{N}{2^L}*\\frac{N}{2^L}*h+h+(F-1)(h*h+h)$$\n",
    "total fully connected parameters.\n",
    "\n",
    "Finally, the last layer has $h$ inputs, $n^{(o)}$ outputs, and $n^{(0)}$ biases\n",
    "$$h*n^{(o)}+n^{(o)}$$\n",
    "\n",
    "This means we have a grand total of\n",
    "$$(3*k*k + 1)*d + L* \\left(d*k*k+1\\right)*d+d*\\frac{N}{2^L}*\\frac{N}{2^L}*h+h+(F-1)(h*h+h)+h*n^{(o)}+n^{(o)}$$\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    " Let B be a NumPy array storing the confusion matrix for a multiclass classifier. Write code to compute\n",
    "(a) The overall accuracy.\n",
    "\n",
    "(b) The macro average precision.\n",
    "\n",
    "(c) The micro average recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 2 0]\n",
      " [1 1 1]\n",
      " [0 1 2]]\n",
      "[0.66666667 0.25       0.66666667]\n",
      "Accuracy: 0.5000\n",
      "Macro Avg Precision: 0.5278\n",
      "Micro Avg Recall: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Generate a confusion matrix as a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: list or np.array of true labels\n",
    "    - y_pred: list or np.array of predicted labels\n",
    "    - num_classes: int, number of classes\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Confusion matrix of shape (num_classes, num_classes)\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Example usage\n",
    "y_true = [0, 1, 2, 2, 0, 1, 2, 0, 1, 0]  # True labels\n",
    "y_pred = [0, 2, 2, 2, 0, 0, 1, 1, 1, 1]  # Predicted labels\n",
    "num_classes = 3\n",
    "\n",
    "cm = generate_confusion_matrix(y_true, y_pred, num_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "# Prediction on the x-axis, True on the y-axis\n",
    "print(cm)\n",
    "\n",
    "# Question starts here (all the previous code was just to generate the confusion matrix)\n",
    "\n",
    "# Note: accuracy = (true positives + true negatives)/(all)\n",
    "#       precision = (true positives) / (true positives + false positives)\n",
    "#       recall = (true positives) / (true positives + false negatives)\n",
    "\n",
    "# Macro = average the precision/recall of each class (equally weigh each class)\n",
    "# Micro = calculate precision/recall by total appearance number (don't equally weight each class)\n",
    "# Micro is stupid cuz it's all just overall accuracy\n",
    "\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "precision_per_class = np.diag(cm) / np.sum(cm, axis=0)\n",
    "#print(precision_per_class)\n",
    "macro_avg_precision = np.mean(precision_per_class)\n",
    "\n",
    "# Note that micro recall and micro precision are both the exact same as overall accuracy lmao\n",
    "micro_recall = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Avg Precision: {macro_avg_precision:.4f}\")\n",
    "print(f\"Micro Avg Recall: {micro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Suppose a convolutional neural network has convolutions that cover $k\\times k$ pixels. After $L$ levels\n",
    "of convolutions, how many pixels are influenced by the color values at pixel location $(x, y)$\n",
    "in the input image.  Ignore any boundary effects. Here, the feature values at location $(u, v)$\n",
    "are “influenced by” location $(x, y)$ if a change in the R,G,B values at $(x, y)$ could potentially\n",
    "change the activations at $(u, v)$.\n",
    "\n",
    "Suppose a $2\\times2$ max pooling operation is inserted after every 2nd convolution layer. How does\n",
    "this change the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within 1 convolution, $(x,y)$ affects $k*k$ pixels, or every surrounding pixel that uses it in their convolution.\n",
    "\n",
    "Within 2 convolutions, $(x,y)$ affects $2k*2k$ pixels.\n",
    "\n",
    "We extend this to $L$ convolutions, where $(x,y)$ affects $Lk*Lk$ pixels.\n",
    "\n",
    "If pooling is inserted after every 2nd convolution layer, we just divide the number of affected pixels by 4 every 2nd convolution layer. The only thing that affects the number of pixels is the reducing of the dimensionality. The pooling operation itself (the max function) doesn't affect the number of affected pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. \n",
    "Suppose we didn’t keep a separate validation set and only used the training set to monitor\n",
    "training performance. What can go wrong during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting. The model will learn to predict the noise in the training set, regardless of whether it is reflective of data outside of the training set.\n",
    "\n",
    "The validation set ensures that we have a representative sample of data outside of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "Let $B$ be a 2D NumPy array of size $N \\times K$ where $N$ is the number of image retrieval queries, $K$\n",
    "is the number of images retrieved for each query, and $B[q, j]$ is 1 if the $j$-th retrieved image\n",
    "for query $q$ is “correct” — has the correct category. Write code to compute (a) the average\n",
    "precision for row q and then (b) the mean average precision across all queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0 0 0 1 1 1]\n",
      " [1 0 0 0 0 1 1 0 1 1]\n",
      " [0 1 0 0 0 1 1 1 1 0]\n",
      " [0 1 1 0 1 0 1 1 1 1]\n",
      " [0 1 1 0 1 1 0 0 1 1]]\n",
      "[0.6 0.5 0.5 0.7 0.6]\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "def generate_B(N, K, correctness_prob=0.5):\n",
    "    \"\"\"\n",
    "    Generate a random B matrix of size N x K where each entry is 1 with probability correctness_prob.\n",
    "    \n",
    "    :param N: Number of queries/classes\n",
    "    :param K: Number of retrieved images per query\n",
    "    :param correctness_prob: Probability that a retrieved image is correct (1)\n",
    "    :return: NumPy array B of shape (N, K)\n",
    "    \"\"\"\n",
    "    return (np.random.rand(N, K) < correctness_prob).astype(int)\n",
    "\n",
    "# Example usage\n",
    "N, K = 5, 10  # Adjust as needed\n",
    "B = generate_B(N, K)\n",
    "print(B)\n",
    "\n",
    "average_precisions_per_row = np.mean(B, axis = 1)\n",
    "\n",
    "average_precision = np.mean(average_precisions_per_row)\n",
    "print(average_precisions_per_row)\n",
    "print(average_precision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
